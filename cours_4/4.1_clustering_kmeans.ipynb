{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a31821e",
   "metadata": {},
   "source": [
    "# Clustering avec k-means\n",
    "\n",
    "Dans ce cours, nous allons utiliser un des algorithmes d'apprentissage non-supervisé. Les algorithmes non-supervisés n'utilisent pas de cible (label) ; au lieu de cela, leur objectif est d'apprendre certaines propriétés des données, de représenter la structure des caractéristiques d'une certaine manière.\n",
    "Le clustering signifie simplement l'attribution de points de données à des groupes en fonction de la similitude des points les uns par rapport aux autres. Il est important de se rappeler que cette fonctionnalité de cluster est catégorique.\n",
    "\n",
    "Il existe de nombreux algorithmes de clustering. Ils diffèrent principalement par la manière dont ils mesurent la « similitude » ou la « proximité » et par les types de fonctionnalités avec lesquels ils fonctionnent. L'algorithme que nous utiliserons, k-means, est intuitif et facile à appliquer. Selon votre application, un autre algorithme peut être plus approprié.\n",
    "\n",
    "L'algorithme **K-means** mesure la similarité en utilisant la distance euclidienne. Il crée des clusters en plaçant un certain nombre de points, appelés centroïdes, à l'intérieur de l'espace des caractéristiques. Chaque point de l'ensemble de données est affecté au cluster du centroïde le plus proche. Le \"k\" dans \"k-means\" est le nombre de centroïdes (c'est-à-dire de clusters) qu'il crée. Vous définissez le k vous-même (c'est un paramètre à passer à l'algorithme).\n",
    "\n",
    "Prenez le temps de regarder la documentation du k-means dans scikit learn, surtout la liste des paramètres (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208b100",
   "metadata": {},
   "source": [
    "Examinons comment l'algorithme k-means apprend les clusters. Nous nous concentrerons sur trois paramètres de l'implémentation de scikit-learn : *n_clusters, max_iter et n_init*.\n",
    "\n",
    "C'est un processus simple en deux étapes. L'algorithme commence par initialiser au hasard un nombre prédéfini (n_clusters) de centroïdes. Il itère ensuite sur ces deux opérations :\n",
    "\n",
    "    1- attribuer des points au centre de gravité du cluster le plus proche\n",
    "    2- déplacer chaque centroïde pour minimiser la distance à ses points\n",
    "\n",
    "Il itère sur ces deux étapes jusqu'à ce que les centroïdes ne bougent plus, ou jusqu'à ce qu'un nombre maximum d'itérations soit écoulé (max_iter).\n",
    "\n",
    "Il arrive souvent que la position aléatoire initiale des centroïdes se termine par un mauvais regroupement. Pour cette raison, l'algorithme se répète un certain nombre de fois (n_init) et renvoie le clustering qui a la plus petite distance totale entre chaque point et son centroïde, le clustering optimal.\n",
    "\n",
    "Vous devrez peut-être augmenter le *max_iter* pour un grand nombre de clusters ou *n_init* pour un ensemble de données complexe. Ordinairement, le seul paramètre que vous devrez choisir vous-même est *n_clusters* (c'est-à-dire *k*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd685df",
   "metadata": {},
   "source": [
    "## Exemple - Logement en Californie\n",
    "\n",
    "En tant que caractéristiques spatiales, la « latitude » et la « longitude » de California Housing sont des candidats naturels pour le regroupement des k-moyennes. Dans cet exemple, nous allons les regrouper avec « MedInc » (revenu médian) pour créer des segments économiques dans différentes régions de Californie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144439ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"../Data/data2/FE-Course-Data/housing.csv\")\n",
    "X = df.loc[:, [\"MedInc\", \"Latitude\", \"Longitude\"]]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81fb1c",
   "metadata": {},
   "source": [
    "Étant donné que le clustering k-means est sensible à l'échelle, il peut être judicieux de redimensionner ou de normaliser les données avec des valeurs extrêmes. Nos fonctionnalités sont déjà à peu près à la même échelle, nous les laisserons donc telles quelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086616d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster feature\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfdfd4",
   "metadata": {},
   "source": [
    "Examinons maintenant quelques *plots* pour voir à quel point cela a été efficace. Tout d'abord, un nuage de points qui montre la répartition géographique des grappes. Il semble que l'algorithme ait créé des segments distincts pour les zones à revenu élevé sur les côtes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd2468",
   "metadata": {},
   "source": [
    "## Exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a82952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv(\"../Data/data2/FE-Course-Data/ames.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1895ec",
   "metadata": {},
   "source": [
    "L'algorithme k-means est sensible à l'échelle. Cela signifie que nous devons réfléchir à la manière et à l'opportunité de redimensionner nos fonctionnalités, car nous pourrions obtenir des résultats très différents en fonction de nos choix. En règle générale, si les caractéristiques sont déjà directement comparables (comme un résultat de test à des moments différents), alors vous ne voudriez pas redimensionner. D'un autre côté, les fonctionnalités qui ne sont pas à des échelles comparables (comme la taille et le poids) bénéficieront généralement d'une remise à l'échelle. Parfois, le choix ne sera pas clair cependant. Dans ce cas, vous devriez essayer de faire preuve de bon sens, en vous rappelant que les caractéristiques avec des valeurs plus élevées seront pondérées plus fortement.\n",
    "\n",
    "### 1) Scaling Features\n",
    "Considérez les ensembles de fonctionnalités suivants. Pour chacun, décidez si :\n",
    "\n",
    "   - ils devraient certainement être redimensionnés,\n",
    "   - ils ne doivent absolument pas être redimensionnés, ou\n",
    "   - soit peut être raisonnable\n",
    "   \n",
    "Features:\n",
    "\n",
    "   1. `Latitude` and `Longitude` of cities in California\n",
    "   2. `Lot Area` and `Living Area` of houses in Ames, Iowa\n",
    "   3. `Number of Doors` and `Horsepower` of a 1989 model car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24378ac",
   "metadata": {},
   "source": [
    "###  2) Create a Feature of Cluster Labels\n",
    "\n",
    "Création d'un clustering k-means avec les paramètres suivants :\n",
    "\n",
    "   - features: `LotArea`, `TotalBsmtSF`, `FirstFlrSF`, `SecondFlrSF`, `GrLivArea`\n",
    "   - number of clusters: 10\n",
    "   - iterations: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9530b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Define a list of the features to be used for the clustering\n",
    "features = ____\n",
    "\n",
    "\n",
    "# Standardize\n",
    "X_scaled = X.loc[:, features]\n",
    "X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Fit the KMeans model to X_scaled and create the cluster labels\n",
    "kmeans = KMeans(____, random_state=0)\n",
    "X[\"Cluster\"] = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f906f",
   "metadata": {},
   "source": [
    "Vous pouvez exécuter cette cellule pour voir le résultat du regroupement, si vous le souhaitez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = X.copy()\n",
    "Xy[\"Cluster\"] = Xy.Cluster.astype(\"category\")\n",
    "Xy[\"SalePrice\"] = y\n",
    "sns.relplot(\n",
    "    x=\"value\", y=\"SalePrice\", hue=\"Cluster\", col=\"variable\",\n",
    "    height=4, aspect=1, facet_kws={'sharex': False}, col_wrap=3,\n",
    "    data=Xy.melt(\n",
    "        value_vars=features, id_vars=[\"SalePrice\", \"Cluster\"],\n",
    "    ),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e54404",
   "metadata": {},
   "source": [
    "La méthode `score_dataset` évaluera votre modèle XGBoost avec cette nouvelle fonctionnalité ajoutée aux données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e2c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
